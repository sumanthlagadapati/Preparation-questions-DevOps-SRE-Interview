## üîπ Kubernetes Interview Q\&A

### 1. **What is the difference between a Deployment and a StatefulSet in Kubernetes?**

* **Deployment**: For **stateless apps** (e.g., web servers, APIs). Pods are interchangeable, no fixed identity.
* **StatefulSet**: For **stateful apps** (e.g., databases, Kafka, Zookeeper). Pods have **sticky identities (pod-0, pod-1, ‚Ä¶)**, ordered startup/termination, and persistent storage.

---

### 2. **When should you use a StatefulSet instead of a Deployment?**

* When pods require:

  * **Stable network identity** (same hostname).
  * **Stable storage** (persistent volume per pod).
  * **Ordered deployment/termination** (e.g., databases, message brokers).
* Example: **Cassandra, Kafka, MongoDB clusters** ‚Üí need StatefulSet.

---

### 3. **Can you attach a volume to a Deployment? If yes, how is it different from a StatefulSet?**

* Yes, Deployments can use **PVCs**. But:

  * **Deployment PVCs** are usually **shared** or **dynamic** (same volume for multiple replicas if RWX).
  * **StatefulSet PVCs** are **per-pod unique** (PVC per replica), ensuring persistent data isolation.

---

### 4. **What could cause a StatefulSet pod to fail when rescheduled to a different availability zone?**

* **Zonal PV binding**: If a PV is created in AZ-1, pod rescheduled to AZ-2 cannot mount it.
* **Fix**: Use **multi-AZ storage classes** (EBS CSI with `volumeBindingMode: WaitForFirstConsumer` or EFS/FSx).

---

### 5. **How do PV/PVC behave across zones in EKS or Kubernetes in general?**

* **EBS volumes** ‚Üí Zonal (only attachable in the same AZ).
* **EFS/FSx** ‚Üí Regional (accessible across multiple AZs).
* Best practice in EKS: Use **StorageClass with `WaitForFirstConsumer`** so PV is provisioned in the AZ where the pod is scheduled.

---

### 6. **What is a DaemonSet and when would you use it?**

* DaemonSet ensures **one pod per node**.
* Common use cases:

  * **Logging agents** (Fluentd, Filebeat).
  * **Monitoring agents** (Datadog, Prometheus Node Exporter).
  * **Networking plugins** (CNI, kube-proxy).

---

### 7. **If you want two pods per node (instead of one), what alternatives to DaemonSet can you use?**

* Use a **Deployment** with `affinity` rules and `podAntiAffinity` to ensure two replicas land on each node.
* Or use **DaemonSet with multiple containers per pod** (not ideal if you need pod-level isolation).

---

### 8. **What is a Pod Disruption Budget (PDB) and how is it useful?**

* **PDB defines min available pods during voluntary disruptions** (node drain, upgrade).
* Example:

  ```yaml
  apiVersion: policy/v1
  kind: PodDisruptionBudget
  spec:
    minAvailable: 2
    selector:
      matchLabels:
        app: my-app
  ```
* Ensures high availability during cluster maintenance.

---

### 9. **How do you handle certificate rotation in on-prem Kubernetes clusters?**

* Options:

  * Use `kubeadm cert renew` for cluster certificates.
  * Automate with **cert-manager** for app TLS certs.
  * Rotate kubelet client/server certs by restarting kubelet.
* Challenge: Downtime risk if certs expire unnoticed ‚Üí solved via **monitoring + automation**.

---

### 10. **What are the challenges with scheduling pods in a multi-node, multi-AZ setup?**

* Challenges:

  * **Zonal PVs** restrict pod rescheduling.
  * **Cross-AZ latency** for DB workloads.
  * **Uneven pod distribution** without affinity rules.
* Fixes: Use **TopologySpreadConstraints**, **PodAntiAffinity**, and **multi-AZ aware StorageClasses**.

---

### 11. **How does the Kubernetes scheduler decide where to place pods?**

* Factors considered:

  * **Resource requests** (CPU/memory).
  * **Affinity/anti-affinity** rules.
  * **Taints & tolerations**.
  * **TopologySpreadConstraints** for AZ balancing.
* Scheduler score = best fit node ‚Üí pod is placed.

---

### 12. **What happens when a StatefulSet pod cannot mount its volume after moving to another node?**

* Likely cause: **EBS volume is stuck in old AZ**.
* Pod fails until it can access a valid PV.
* Fixes:

  * Use **regional storage (EFS/FSx)**.
  * Or ensure **StorageClass with WaitForFirstConsumer** aligns PVC with pod AZ.

---
============================================================================================================================

==============================================================================================================================

Perfect üëç Let‚Äôs turn your curated list into a **hands-on learning roadmap + resume-ready showcase**.

---

# üöÄ DevOps & SRE Chaos Engineering Roadmap

This roadmap takes you from command-line debugging to cloud chaos and postmortem culture in **6 weeks of progressive labs**.

---

## üìÖ Week-by-Week Plan

### **Week 1 ‚Äì Linux & Debugging Foundations**

* üñ• **Explainshell** ‚Üí Break down Bash/awk/sed commands.
* üñ• **Linux Strace Guide** ‚Üí Trace syscalls, debug when logs lie.
* üñ• **eBPF Academy** ‚Üí Kernel-level observability (sockets, latency, network drops).

‚úÖ Outcome: Comfort with advanced Linux troubleshooting.

---

### **Week 2 ‚Äì Kubernetes Resilience Labs**

* ‚ò∏ **KubeSim** ‚Üí Browser-based chaos drills: crash etcd, break kube-proxy.
* ‚ò∏ **etcd Playground** ‚Üí Practice restoring a corrupted Kubernetes brain.
* ‚ò∏ **Container Security Playground (Katacoda)** ‚Üí Secure workloads hands-on.
* ‚ò∏ **Helm Best Practices Repo** ‚Üí Fix anti-patterns beyond `helm install`.

‚úÖ Outcome: Experience handling K8s outages + secure deployments.

---

### **Week 3 ‚Äì Cloud Chaos Engineering (AWS + Azure)**

* ‚òÅÔ∏è **AWS Fault Injection Simulator** ‚Üí Inject EC2/RDS/EKS failures.
* ‚òÅÔ∏è **Azure Well-Architected Review** ‚Üí Learn high availability, security, cost trade-offs.
* ‚òÅÔ∏è **Karpenter Workshop** ‚Üí AWS autoscaling + cost explosion lessons.

‚úÖ Outcome: Cloud-native failure recovery + cost optimization strategies.

---

### **Week 4 ‚Äì Networking & Load Balancing**

* üåê **HAProxy Config Explorer** ‚Üí Visualize configs, detect bottlenecks before prod breaks.

‚úÖ Outcome: Hands-on network resiliency tuning.

---

### **Week 5 ‚Äì Observability & Dashboards**

* üìä **PromCat** ‚Üí Deploy curated Prometheus dashboards by Sysdig.
* üìä Integrate with Grafana for golden signals: latency, errors, saturation, traffic.

‚úÖ Outcome: Build production-grade observability stack.

---

### **Week 6 ‚Äì Incident Culture & Postmortems**

* ‚ö° **Postmortem Templates (Google & Netflix)** ‚Üí Write RCAs like FAANG.
* ‚ö° **Chaos Monkey (Netflix OSS)** ‚Üí Run resilience experiments.
* ‚ö° **Incident.io Learning Hub** ‚Üí Learn modern incident response & escalation.

‚úÖ Outcome: Master resilience culture + RCA writing.

---

# üìÑ Resume-Style Project Showcase

**Chaos Engineering & Reliability Labs (Self-Driven Projects)**

* Designed chaos simulations using **AWS Fault Injection Simulator**, **KubeSim**, and **Chaos Monkey** to validate resilience of cloud and Kubernetes workloads.
* Practiced **etcd recovery drills** and Kubernetes troubleshooting under failure conditions, improving cluster availability knowledge.
* Implemented **container workload security labs** (Katacoda) and applied **Helm best practices** to avoid production misconfigurations.
* Explored **Linux syscall tracing (strace, eBPF)** to debug kernel-level failures in real time.
* Built **Prometheus + Grafana observability stack** using **PromCat dashboards**, covering golden signals (latency, error rates, saturation, traffic).
* Reviewed **Azure Well-Architected Framework** and tuned **AWS Karpenter autoscaling** for cost vs. performance trade-offs.
* Authored **Google/Netflix-style postmortems**, leveraging **Incident.io frameworks** to simulate enterprise RCA practices.
* Analyzed **HAProxy load balancer configs** to detect bottlenecks and optimize L7 routing policies.

===============================================================================================================================================================================
